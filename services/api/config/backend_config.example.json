{
  "componentDir": "../frontend/src/components/generated_by_agent",
  "logDir": "../logs",
  "workspaceDir": "../output",
  "llmProvider": "ollama",
  "apiKey": "",
  "defaultOllamaModel": "mistral",
  "defaultOpenAIModel": "gpt-4",
  "OLLAMA_BASE_URL": "http://localhost:11434",
  "//": "--- How to use this file ---",
  "//": "1. Rename this file to 'backend_config.json' in the same directory.",
  "//": "2. Modify the paths above to point to your desired locations.",
  "//": "   - Paths can be absolute (e.g., '/var/roadrunner/logs' or 'C:\\Users\\YourUser\\roadrunner_output').",
  "//": "   - Paths can be relative (e.g., '../my_custom_logs' or './roadmaps/custom_front').",
  "//": "     Relative paths are resolved from the 'roadrunner/backend' directory.",
  "//": "",
  "//": "--- Path Descriptions ---",
  "//": "'componentDir':       Directory for frontend components generated by the agent (e.g., Vue components via CodeGeneratorTool).",
  "//": "                      Default: '../frontend/src/components/generated_by_agent' (relative to roadrunner/backend).",
  "//": "                      Env Var: RR_COMPONENT_DIR",
  "//": "",
  "//": "'logDir':             Directory where Roadrunner backend execution logs are stored.",
  "//": "                      Default: '../logs' (relative to roadrunner/backend).",
  "//": "                      Env Var: RR_LOG_DIR",
  "//": "",
  "//": "'workspaceDir':       The primary output directory for tasks, generated files, and git operations.",
  "//": "                      This overrides the electron-store setting and the legacy ROADRUNNER_WORKSPACE_DIR env var if set.",
  "//": "                      Default: '../output' (relative to roadrunner/backend).",
  "//": "                      Env Var: RR_WORKSPACE_DIR",
  "//": "",
  "//": "--- New LLM Settings ---",
  "//": "'llmProvider':        The LLM provider to use. Can be 'ollama' or 'openai'.",
  "//": "                      Default: 'ollama'",
  "//": "                      Env Var: RR_LLM_PROVIDER",
  "//": "",
  "//": "'apiKey':             The API key for the LLM provider (e.g. OpenAI).",
  "//": "                      Default: ''",
  "//": "                      Env Var: RR_API_KEY",
  "//": "",
  "//": "'defaultOllamaModel': The default Ollama model to use.",
  "//": "                      Default: 'mistral'",
  "//": "                      Env Var: RR_DEFAULT_OLLAMA_MODEL",
  "//": "",
  "//": "'defaultOpenAIModel': The default OpenAI model to use.",
  "//": "                      Default: 'gpt-4'",
  "//": "                      Env Var: RR_DEFAULT_OPENAI_MODEL",
  "//": "",
  "//": "'OLLAMA_BASE_URL':    The base URL for the Ollama API.",
  "//": "                      Default: 'http://localhost:11434'",
  "//": "                      Env Var: OLLAMA_BASE_URL",
  "//": "",
  "//": "--- Configuration Precedence ---",
  "//": "1. Environment Variables (e.g., RR_COMPONENT_DIR)",
  "//": "2. This 'backend_config.json' file (if it exists and keys are set)",
  "//": "3. Default values coded into server.js"
}
